 ğŸ“„ RAG using VectorDB (Chroma) with Groq LLM

A **Retrieval-Augmented Generation (RAG)** system built using **Flowise**, **Chroma Vector Database**, **HuggingFace Embeddings**, and **Groq LLM**.  
This project enables **PDF-based conversational question answering** with memory, caching, and ultra-fast inference.

---



## ğŸš€ Features

- ğŸ“š PDF document ingestion  
- âœ‚ï¸ Recursive text chunking  
- ğŸ§  Embeddings using HuggingFace  
- ğŸ—„ï¸ Chroma Vector Database for similarity search  
- âš¡ High-speed inference using Groq LPU  
- ğŸ’¬ Conversational memory (chat history aware)  
- ğŸ” Follow-up question rephrasing  
- ğŸ§¾ Context-grounded answers (no hallucinations)  
- ğŸ§  In-memory LLM response caching  





## ğŸ—ï¸ Architecture
PDF File
   â†“
   
Text Splitter (RecursiveCharacterTextSplitter)
   â†“
   
HuggingFace Embeddings
   â†“
   
Chroma Vector Store
   â†“
   
Retriever
   â†“
   
Conversational Retrieval QA Chain
   â†“
   
Groq LLM (Chat Model)






ğŸ“‚ Project Files
.

â”œâ”€â”€ RAG USING VECTORDB Chatflow.json

â”œâ”€â”€ README.md




âš™ï¸ Setup Instructions
1ï¸âƒ£ Install Flowise
npm install -g flowise


2ï¸âƒ£ Start Flowise
flowise start

Open ğŸ‘‰ http://localhost:3000



3ï¸âƒ£ Import Chatflow

Go to Flowise UI

Click Import

Upload RAG USING VECTORDB Chatflow.json



4ï¸âƒ£ Configure Credentials

Add the following credentials in Flowise:

ğŸ”¹ Groq API

Provider: Groq

API Key: GROQ_API_KEY


ğŸ”¹ HuggingFace API

Provider: HuggingFace

API Key: HF_API_KEY

