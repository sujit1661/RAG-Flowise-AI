 ğŸ“„ RAG using VectorDB (Chroma) with Groq LLM

A **Retrieval-Augmented Generation (RAG)** system built using **Flowise**, **Chroma Vector Database**, **HuggingFace Embeddings**, and **Groq LLM**.  
This project enables **PDF-based conversational question answering** with memory, caching, and ultra-fast inference.

---



## ğŸš€ Features

- ğŸ“š PDF document ingestion  
- âœ‚ï¸ Recursive text chunking  
- ğŸ§  Embeddings using HuggingFace  
- ğŸ—„ï¸ Chroma Vector Database for similarity search  
- âš¡ High-speed inference using Groq LPU  
- ğŸ’¬ Conversational memory (chat history aware)  
- ğŸ” Follow-up question rephrasing  
- ğŸ§¾ Context-grounded answers (no hallucinations)  
- ğŸ§  In-memory LLM response caching  





## ğŸ—ï¸ Architecture
PDF File
   â†“
Text Splitter (RecursiveCharacterTextSplitter)
   â†“
HuggingFace Embeddings
   â†“
Chroma Vector Store
   â†“
Retriever
   â†“
Conversational Retrieval QA Chain
   â†“
Groq LLM (Chat Model)




ğŸ“‚ Project Files
.
â”œâ”€â”€ RAG USING VECTORDB Chatflow.json
â”œâ”€â”€ README.md
