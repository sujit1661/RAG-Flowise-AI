# ğŸ“„ RAG using VectorDB (Chroma) with Groq LLM

A **Retrieval-Augmented Generation (RAG)** system built using **Flowise**, **Chroma Vector Database**, **HuggingFace Embeddings**, and **Groq LLM**.  
This project enables **PDF-based conversational question answering** with memory, caching, and ultra-fast inference.

---

## ğŸš€ Features

- ğŸ“š **PDF document ingestion** â€“ Upload and process PDF files easily  
- âœ‚ï¸ **Recursive text chunking** â€“ Break large documents into manageable chunks  
- ğŸ§  **Embeddings using HuggingFace** â€“ High-quality text vectorization  
- ğŸ—„ï¸ **Chroma Vector Database** â€“ Efficient similarity search for retrieval  
- âš¡ **High-speed inference using Groq LPU** â€“ Ultra-fast LLM responses  
- ğŸ’¬ **Conversational memory** â€“ Maintains chat history context  
- ğŸ” **Follow-up question rephrasing** â€“ Context-aware query reformulation  
- ğŸ§¾ **Context-grounded answers** â€“ Reduces hallucinations using retrieved context  
- ğŸ§  **In-memory caching** â€“ Faster responses for repeated or similar queries  

---

## ğŸ—ï¸ Architecture

High-level RAG pipeline:

    PDF File
        â†“
    Text Splitter (RecursiveCharacterTextSplitter)
        â†“
    HuggingFace Embeddings
        â†“
    Chroma Vector Store
        â†“
    Retriever
        â†“
    Conversational Retrieval QA Chain
        â†“
    Groq LLM (Chat Model)

---

## ğŸ“‚ Project Files

    .
    â”œâ”€â”€ RAG USING VECTORDB Chatflow.json
    â””â”€â”€ README.md

---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Install Flowise

Install Flowise globally using npm:

    npm install -g flowise

### 2ï¸âƒ£ Start Flowise

Start the Flowise server:

    flowise start

Then open Flowise in your browser:

    http://localhost:3000

---

### 3ï¸âƒ£ Import Chatflow

1. Open the **Flowise UI** in your browser.  
2. Click on **Import** in the top-right (or appropriate) menu.  
3. Select and upload: **RAG USING VECTORDB Chatflow.json**.  

This will create the full RAG pipeline for you.

---

### 4ï¸âƒ£ Configure Credentials

In the Flowise UI, go to **Credentials** and add:

#### ğŸ”¹ Groq API

- **Provider**: Groq  
- **API Key**: `GROQ_API_KEY`  

#### ğŸ”¹ HuggingFace API

- **Provider**: HuggingFace  
- **API Key**: `HF_API_KEY`  

Make sure both keys are active and have sufficient quota.

---

## ğŸ’¡ Usage

1. **Upload PDFs**  
   - In the configured chatflow, use the file upload node/step to add one or more PDF documents.  

2. **Ask Questions**  
   - Use the chat interface in Flowise to ask natural language questions about your PDFs.  

3. **Retrieval + Generation**  
   - The system:
     - Splits PDFs into chunks  
     - Creates embeddings with HuggingFace  
     - Stores and retrieves chunks via Chroma  
     - Uses Groq LLM to generate answers grounded in retrieved context  

4. **Conversational Follow-ups**  
   - Ask follow-up questions; the system:
     - Uses conversational memory  
     - Optionally rephrases follow-up questions  
     - Retrieves new relevant context and responds accordingly  

---

## ğŸ“Œ Notes

- Ensure both **Groq** and **HuggingFace** API keys are valid and correctly set in Flowise.  
- For **large PDFs**, embedding and indexing may take time on first run.  
- In-memory caching improves performance for:
  - Repeated questions  
  - Similar queries over the same document set  
- The system is designed to **minimize hallucinations** by always grounding responses in retrieved context from Chroma.  

---

## ğŸŒ References

- **Flowise** â€“ Low-code / no-code LLM workflow builder  
- **Chroma Vector Database** â€“ Open-source embedding database for similarity search  
- **HuggingFace Embeddings** â€“ Pretrained models for text embeddings  
- **Groq LLM** â€“ High-speed LPU-accelerated large language model inference
